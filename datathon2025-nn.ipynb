{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.352151</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.216204</td>\n",
       "      <td>-0.395204</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-1.562721</td>\n",
       "      <td>0.133379</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.587964</td>\n",
       "      <td>0.079120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-0.493553</td>\n",
       "      <td>-0.419741</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.352151</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096753</td>\n",
       "      <td>0.630613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.924252</td>\n",
       "      <td>0.133379</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.723343</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.526417</td>\n",
       "      <td>0.030279</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.449513</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-1.562721</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.352151</td>\n",
       "      <td>3</td>\n",
       "      <td>0.761353</td>\n",
       "      <td>1.348041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>0.133379</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>1455</td>\n",
       "      <td>0.352151</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.344154</td>\n",
       "      <td>-0.585509</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.697507</td>\n",
       "      <td>-0.419741</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>1456</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.835558</td>\n",
       "      <td>1.107339</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-1.562721</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>1457</td>\n",
       "      <td>0.723343</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.149853</td>\n",
       "      <td>-0.137177</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>-0.493553</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>1458</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096753</td>\n",
       "      <td>0.118777</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.295363</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-0.860725</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>1459</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.288750</td>\n",
       "      <td>0.190587</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-0.095492</td>\n",
       "      <td>0.133379</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1094 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  MSSubClass  MSZoning  LotFrontage   LotArea  Street  \\\n",
       "0              0    0.352151         3    -0.216204 -0.395204       1   \n",
       "1              1   -5.199338         3     0.587964  0.079120       1   \n",
       "2              2    0.352151         3    -0.096753  0.630613       1   \n",
       "3              3    0.723343         3    -0.526417  0.030279       1   \n",
       "4              4    0.352151         3     0.761353  1.348041       1   \n",
       "...          ...         ...       ...          ...       ...     ...   \n",
       "1089        1455    0.352151         3    -0.344154 -0.585509       1   \n",
       "1090        1456   -5.199338         3     0.835558  1.107339       1   \n",
       "1091        1457    0.723343         3    -0.149853 -0.137177       1   \n",
       "1092        1458   -5.199338         3    -0.096753  0.118777       1   \n",
       "1093        1459   -5.199338         3     0.288750  0.190587       1   \n",
       "\n",
       "      LotShape  LandContour  Utilities  LotConfig  ...  EnclosedPorch  \\\n",
       "0            3            3          0          4  ...      -5.199338   \n",
       "1            3            3          0          2  ...      -5.199338   \n",
       "2            0            3          0          4  ...      -5.199338   \n",
       "3            0            3          0          0  ...       2.449513   \n",
       "4            0            3          0          2  ...      -5.199338   \n",
       "...        ...          ...        ...        ...  ...            ...   \n",
       "1089         3            3          0          4  ...      -5.199338   \n",
       "1090         3            3          0          4  ...      -5.199338   \n",
       "1091         3            3          0          4  ...      -5.199338   \n",
       "1092         3            3          0          4  ...       1.295363   \n",
       "1093         3            3          0          4  ...      -5.199338   \n",
       "\n",
       "      3SsnPorch  ScreenPorch  PoolArea   MiscVal    MoSold    YrSold  \\\n",
       "0     -5.199338    -5.199338 -5.199338 -5.199338 -1.562721  0.133379   \n",
       "1     -5.199338    -5.199338 -5.199338 -5.199338 -0.493553 -0.419741   \n",
       "2     -5.199338    -5.199338 -5.199338 -5.199338  0.924252  0.133379   \n",
       "3     -5.199338    -5.199338 -5.199338 -5.199338 -1.562721 -5.199338   \n",
       "4     -5.199338    -5.199338 -5.199338 -5.199338  5.199338  0.133379   \n",
       "...         ...          ...       ...       ...       ...       ...   \n",
       "1089  -5.199338    -5.199338 -5.199338 -5.199338  0.697507 -0.419741   \n",
       "1090  -5.199338    -5.199338 -5.199338 -5.199338 -1.562721  5.199338   \n",
       "1091  -5.199338    -5.199338 -5.199338  5.199338 -0.493553  5.199338   \n",
       "1092  -5.199338    -5.199338 -5.199338 -5.199338 -0.860725  5.199338   \n",
       "1093  -5.199338    -5.199338 -5.199338 -5.199338 -0.095492  0.133379   \n",
       "\n",
       "      SaleType  SaleCondition  SalePrice  \n",
       "0            8              4     208500  \n",
       "1            8              4     181500  \n",
       "2            8              4     223500  \n",
       "3            8              0     140000  \n",
       "4            8              4     250000  \n",
       "...        ...            ...        ...  \n",
       "1089         8              4     175000  \n",
       "1090         8              4     210000  \n",
       "1091         8              4     266500  \n",
       "1092         8              4     142125  \n",
       "1093         8              4     147500  \n",
       "\n",
       "[1094 rows x 75 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"assets/preprocessed_train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "y = train_df[\"SalePrice\"]\n",
    "X = train_df.drop(columns=[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6       307000\n",
       "789     180000\n",
       "1049    193000\n",
       "837     117000\n",
       "661     200500\n",
       "         ...  \n",
       "330      90350\n",
       "466     118500\n",
       "121     215000\n",
       "1044    281213\n",
       "860     174500\n",
       "Name: SalePrice, Length: 875, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 68\u001b[0m\n\u001b[0;32m     62\u001b[0m l2_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(param \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Total loss with regularization\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# total_loss = mse_loss + l1_lambda * l1_loss + l2_lambda * l2_loss\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Less frequent logging\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_fn\u001b[38;5;241m.\u001b[39m_register_hook_dict(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RemovableHandle\n",
      "File \u001b[1;32mc:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m attach_logging_hooks \u001b[38;5;241m=\u001b[39m log\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n\u001b[1;32m--> 823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the improved regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  # Increased neurons\n",
    "        self.hidden2 = nn.Linear(20, 10)  # Additional hidden layer\n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters (updated)\n",
    "l1_lambda = 0.001\n",
    "l2_lambda = 0.001\n",
    "learning_rate = 0.001\n",
    "epochs = 500  # More training\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with GPU\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_torch)\n",
    "    \n",
    "    # Compute base MSE loss\n",
    "    mse_loss = criterion(y_pred, y_train_torch)\n",
    "    rmse_loss = torch.sqrt(mse_loss)\n",
    "\n",
    "    # Compute L1 and L2 loss\n",
    "    l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "    l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "\n",
    "    # Total loss with regularization\n",
    "    # total_loss = mse_loss + l1_lambda * l1_loss + l2_lambda * l2_loss\n",
    "\n",
    "    # Backpropagation\n",
    "    rmse_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:  # Less frequent logging\n",
    "        print(f\"Epoch {epoch}, RMSE: {rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 0, Train RMSE: 48829.39453125\n",
      "Epoch 50, Train RMSE: 2409.87939453125\n",
      "Epoch 100, Train RMSE: 648.390380859375\n",
      "Epoch 150, Train RMSE: 176.7240447998047\n",
      "Epoch 200, Train RMSE: 120.6166000366211\n",
      "Test RMSE: 55847.9921875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train RMSE: 204115.375\n",
      "Epoch 50, Train RMSE: 1024.5865478515625\n",
      "Epoch 100, Train RMSE: 183.5473175048828\n",
      "Epoch 150, Train RMSE: 144.09107971191406\n",
      "Epoch 200, Train RMSE: 106.96619415283203\n",
      "Test RMSE: 60765.32421875\n"
     ]
    }
   ],
   "source": [
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  \n",
    "        self.hidden2 = nn.Linear(20, 10)  \n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use L-BFGS optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Training loop with L-BFGS\n",
    "epochs = 250\n",
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        y_pred = model(X_train_torch)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss = criterion(y_pred, y_train_torch)\n",
    "        rmse_loss = torch.sqrt(mse_loss)\n",
    "        \n",
    "        # Compute L1 and L2 loss\n",
    "        l1_lambda = 0.001\n",
    "        l2_lambda = 0.001\n",
    "        l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "        \n",
    "        # Total loss with regularization\n",
    "        # total_loss = mse_loss + l1_lambda * l1_loss + l2_lambda * l2_loss\n",
    "        \n",
    "        rmse_loss.backward()  # Compute gradients\n",
    "        return rmse_loss\n",
    "\n",
    "    optimizer.step(closure)  # Perform L-BFGS step\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_torch)\n",
    "            train_mse_loss = criterion(y_pred_train, y_train_torch)\n",
    "            train_rmse_loss = torch.sqrt(train_mse_loss)\n",
    "            print(f\"Epoch {epoch}, Train RMSE: {train_rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train RMSE: 204115.59375\n",
      "Epoch 50, Train RMSE: 629.6298828125\n",
      "Test RMSE: 54197.40234375\n"
     ]
    }
   ],
   "source": [
    "# Using L1 norm\n",
    "\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  \n",
    "        self.hidden2 = nn.Linear(20, 10)  \n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use L-BFGS optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Training loop with L-BFGS\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        y_pred = model(X_train_torch)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss = criterion(y_pred, y_train_torch)\n",
    "        rmse_loss = torch.sqrt(mse_loss)\n",
    "        \n",
    "        # Compute L1 and L2 loss\n",
    "        l1_lambda = 0.001\n",
    "        l2_lambda = 0.001\n",
    "        l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "        \n",
    "        # Total loss with regularization\n",
    "        total_loss = mse_loss + l1_lambda * l1_loss\n",
    "        \n",
    "        total_loss.backward()  # Compute gradients\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)  # Perform L-BFGS step\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_torch)\n",
    "            train_mse_loss = criterion(y_pred_train, y_train_torch)\n",
    "            train_rmse_loss = torch.sqrt(train_mse_loss)\n",
    "            print(f\"Epoch {epoch}, Train RMSE: {train_rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train RMSE: 255189008384.0\n",
      "Epoch 50, Train RMSE: 92527.921875\n",
      "Epoch 100, Train RMSE: 87112.78125\n",
      "Epoch 150, Train RMSE: 86264.5546875\n",
      "Epoch 200, Train RMSE: 84034.2734375\n",
      "Test RMSE: 20970590.0\n"
     ]
    }
   ],
   "source": [
    "# Using L2 norm\n",
    "\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  \n",
    "        self.hidden2 = nn.Linear(20, 10)  \n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use L-BFGS optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Training loop with L-BFGS\n",
    "epochs = 250\n",
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        y_pred = model(X_train_torch)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss = criterion(y_pred, y_train_torch)\n",
    "        rmse_loss = torch.sqrt(mse_loss)\n",
    "        \n",
    "        # Compute L1 and L2 loss\n",
    "        l1_lambda = 0.001\n",
    "        l2_lambda = 0.001\n",
    "        l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "        \n",
    "        # Total loss with regularization\n",
    "        total_loss = mse_loss + l2_lambda * l2_loss\n",
    "        \n",
    "        total_loss.backward()  # Compute gradients\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)  # Perform L-BFGS step\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_torch)\n",
    "            train_mse_loss = criterion(y_pred_train, y_train_torch)\n",
    "            train_rmse_loss = torch.sqrt(train_mse_loss)\n",
    "            print(f\"Epoch {epoch}, Train RMSE: {train_rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
