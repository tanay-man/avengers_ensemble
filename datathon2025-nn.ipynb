{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "- pandas for data manipulation\n",
    "- numpy for numerical operations\n",
    "- scikit-learn for machine learning\n",
    "- XGBoost for gradient boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Dataset\n",
    "Loading the training data for model development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>AgeWhenSold</th>\n",
       "      <th>RenovatedAgeSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-0.207071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>208500</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-0.091855</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>181500</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.073455</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>223500</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.096864</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "      <td>91</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.375020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>250000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>1455</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.260471</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>175000</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>1456</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.266316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>210000</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>1457</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.147760</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.951415</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>266500</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>1458</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.080133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>142125</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-0.058092</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087658</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>147500</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  MSSubClass  MSZoning  LotFrontage   LotArea  Street  Alley  \\\n",
       "0              0          60         3         65.0 -0.207071       1      0   \n",
       "1              1          20         3         80.0 -0.091855       1      0   \n",
       "2              2          60         3         68.0  0.073455       1      0   \n",
       "3              3          70         3         60.0 -0.096864       1      0   \n",
       "4              4          60         3         84.0  0.375020       1      0   \n",
       "...          ...         ...       ...          ...       ...     ...    ...   \n",
       "1090        1455          60         3         62.0 -0.260471       1      0   \n",
       "1091        1456          20         3         85.0  0.266316       1      0   \n",
       "1092        1457          70         3         66.0 -0.147760       1      0   \n",
       "1093        1458          20         3         68.0 -0.080133       1      0   \n",
       "1094        1459          20         3         75.0 -0.058092       1      0   \n",
       "\n",
       "      LotShape  LandContour  Utilities  ...  PoolQC  Fence  MiscFeature  \\\n",
       "0            3            3          0  ...       0      0            0   \n",
       "1            3            3          0  ...       0      0            0   \n",
       "2            0            3          0  ...       0      0            0   \n",
       "3            0            3          0  ...       0      0            0   \n",
       "4            0            3          0  ...       0      0            0   \n",
       "...        ...          ...        ...  ...     ...    ...          ...   \n",
       "1090         3            3          0  ...       0      0            0   \n",
       "1091         3            3          0  ...       0      3            0   \n",
       "1092         3            3          0  ...       0      1            3   \n",
       "1093         3            3          0  ...       0      0            0   \n",
       "1094         3            3          0  ...       0      0            0   \n",
       "\n",
       "       MiscVal  MoSold  SaleType  SaleCondition  SalePrice  AgeWhenSold  \\\n",
       "0    -0.087658       2         8              4     208500            5   \n",
       "1    -0.087658       5         8              4     181500           31   \n",
       "2    -0.087658       9         8              4     223500            7   \n",
       "3    -0.087658       2         8              0     140000           91   \n",
       "4    -0.087658      12         8              4     250000            8   \n",
       "...        ...     ...       ...            ...        ...          ...   \n",
       "1090 -0.087658       8         8              4     175000            8   \n",
       "1091 -0.087658       2         8              4     210000           32   \n",
       "1092  4.951415       5         8              4     266500           69   \n",
       "1093 -0.087658       4         8              4     142125           60   \n",
       "1094 -0.087658       6         8              4     147500           43   \n",
       "\n",
       "      RenovatedAgeSold  \n",
       "0                    5  \n",
       "1                   31  \n",
       "2                    6  \n",
       "3                   36  \n",
       "4                    8  \n",
       "...                ...  \n",
       "1090                 7  \n",
       "1091                22  \n",
       "1092                 4  \n",
       "1093                14  \n",
       "1094                43  \n",
       "\n",
       "[1095 rows x 80 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"assets/final_preprocessed_train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "Training a neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "y = train_df[\"SalePrice\"]\n",
    "X = train_df.drop(columns=[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Training the machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Training the machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6       307000\n",
       "575     127500\n",
       "821     146000\n",
       "1063    122500\n",
       "905     117000\n",
       "         ...  \n",
       "330     119000\n",
       "466     359100\n",
       "121     215000\n",
       "1044    246578\n",
       "860     149000\n",
       "Name: SalePrice, Length: 876, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "- pandas for data manipulation\n",
    "- numpy for numerical operations\n",
    "- scikit-learn for machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0, RMSE: 203734.984375\n",
      "Epoch 50, RMSE: 203733.828125\n",
      "Epoch 100, RMSE: 203727.71875\n",
      "Epoch 150, RMSE: 203707.875\n",
      "Epoch 200, RMSE: 203664.25\n",
      "Epoch 250, RMSE: 203587.484375\n",
      "Epoch 300, RMSE: 203468.6875\n",
      "Epoch 350, RMSE: 203299.46875\n",
      "Epoch 400, RMSE: 203071.890625\n",
      "Epoch 450, RMSE: 202778.46875\n",
      "Test RMSE: 207442.875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the improved regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  # Increased neurons\n",
    "        self.hidden2 = nn.Linear(20, 10)  # Additional hidden layer\n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters (updated)\n",
    "l1_lambda = 0.001\n",
    "l2_lambda = 0.001\n",
    "learning_rate = 0.001\n",
    "epochs = 500  # More training\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with GPU\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_torch)\n",
    "    \n",
    "    # Compute base MSE loss\n",
    "    mse_loss = criterion(y_pred, y_train_torch)\n",
    "    rmse_loss = torch.sqrt(mse_loss)\n",
    "\n",
    "    # Compute L1 and L2 loss\n",
    "    l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "    l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "\n",
    "    # Total loss with regularization\n",
    "    # total_loss = mse_loss + l1_lambda * l1_loss + l2_lambda * l2_loss\n",
    "\n",
    "    # Backpropagation\n",
    "    rmse_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:  # Less frequent logging\n",
    "        print(f\"Epoch {epoch}, RMSE: {rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "- numpy for numerical operations\n",
    "- scikit-learn for machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "Training a neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0, Train RMSE: 203734.421875\n",
      "Epoch 50, Train RMSE: 928.4501342773438\n",
      "Epoch 100, Train RMSE: 1.6931579113006592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m     rmse_loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rmse_loss\n\u001b[1;32m---> 63\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform L-BFGS step\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanay\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\lbfgs.py:401\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    400\u001b[0m     al[i] \u001b[38;5;241m=\u001b[39m old_stps[i]\u001b[38;5;241m.\u001b[39mdot(q) \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[1;32m--> 401\u001b[0m     \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_dirs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mal\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# multiply by initial Hessian\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# r/d is the final direction\u001b[39;00m\n\u001b[0;32m    405\u001b[0m d \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(q, H_diag)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  \n",
    "        self.hidden2 = nn.Linear(20, 10)  \n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use L-BFGS optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Training loop with L-BFGS\n",
    "epochs = 250\n",
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        y_pred = model(X_train_torch)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss = criterion(y_pred, y_train_torch)\n",
    "        rmse_loss = torch.sqrt(mse_loss)\n",
    "        \n",
    "        # Compute L1 and L2 loss\n",
    "        l1_lambda = 0.001\n",
    "        l2_lambda = 0.001\n",
    "        l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "        \n",
    "        # Total loss with regularization\n",
    "        # total_loss = mse_loss + l1_lambda * l1_loss + l2_lambda * l2_loss\n",
    "        \n",
    "        rmse_loss.backward()  # Compute gradients\n",
    "        return rmse_loss\n",
    "\n",
    "    optimizer.step(closure)  # Perform L-BFGS step\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_torch)\n",
    "            train_mse_loss = criterion(y_pred_train, y_train_torch)\n",
    "            train_rmse_loss = torch.sqrt(train_mse_loss)\n",
    "            print(f\"Epoch {epoch}, Train RMSE: {train_rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using L1 norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "Training a neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0, Train RMSE: 203734.8125\n",
      "Epoch 50, Train RMSE: 2279.405029296875\n",
      "Test RMSE: 69300.046875\n"
     ]
    }
   ],
   "source": [
    "# Using L1 norm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  \n",
    "        self.hidden2 = nn.Linear(20, 10)  \n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use L-BFGS optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Training loop with L-BFGS\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        y_pred = model(X_train_torch)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss = criterion(y_pred, y_train_torch)\n",
    "        rmse_loss = torch.sqrt(mse_loss)\n",
    "        \n",
    "        # Compute L1 and L2 loss\n",
    "        l1_lambda = 0.001\n",
    "        l2_lambda = 0.001\n",
    "        l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "        \n",
    "        # Total loss with regularization\n",
    "        total_loss = mse_loss + l1_lambda * l1_loss\n",
    "        \n",
    "        total_loss.backward()  # Compute gradients\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)  # Perform L-BFGS step\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_torch)\n",
    "            train_mse_loss = criterion(y_pred_train, y_train_torch)\n",
    "            train_rmse_loss = torch.sqrt(train_mse_loss)\n",
    "            print(f\"Epoch {epoch}, Train RMSE: {train_rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using L2 norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "Training a neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train RMSE: 255189008384.0\n",
      "Epoch 50, Train RMSE: 92527.921875\n",
      "Epoch 100, Train RMSE: 87112.78125\n",
      "Epoch 150, Train RMSE: 86264.5546875\n",
      "Epoch 200, Train RMSE: 84034.2734375\n",
      "Test RMSE: 20970590.0\n"
     ]
    }
   ],
   "source": [
    "# Using L2 norm\n",
    "\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the regression model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 20)  \n",
    "        self.hidden2 = nn.Linear(20, 10)  \n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model and move it to GPU\n",
    "input_size = X_train_torch.shape[1]\n",
    "model = RegressionNN(input_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use L-BFGS optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Training loop with L-BFGS\n",
    "epochs = 250\n",
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        y_pred = model(X_train_torch)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss = criterion(y_pred, y_train_torch)\n",
    "        rmse_loss = torch.sqrt(mse_loss)\n",
    "        \n",
    "        # Compute L1 and L2 loss\n",
    "        l1_lambda = 0.001\n",
    "        l2_lambda = 0.001\n",
    "        l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        l2_loss = sum(torch.sum(param ** 2) for param in model.parameters())\n",
    "        \n",
    "        # Total loss with regularization\n",
    "        total_loss = mse_loss + l2_lambda * l2_loss\n",
    "        \n",
    "        total_loss.backward()  # Compute gradients\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)  # Perform L-BFGS step\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_torch)\n",
    "            train_mse_loss = criterion(y_pred_train, y_train_torch)\n",
    "            train_rmse_loss = torch.sqrt(train_mse_loss)\n",
    "            print(f\"Epoch {epoch}, Train RMSE: {train_rmse_loss.item()}\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_torch)\n",
    "    test_mse_loss = criterion(y_test_pred, y_test_torch)\n",
    "    test_rmse_loss = torch.sqrt(test_mse_loss)\n",
    "    print(f\"Test RMSE: {test_rmse_loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
