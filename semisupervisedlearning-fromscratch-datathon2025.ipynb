{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T18:18:36.066582Z",
     "iopub.status.busy": "2025-04-03T18:18:36.066320Z",
     "iopub.status.idle": "2025-04-03T18:18:37.962839Z",
     "shell.execute_reply": "2025-04-03T18:18:37.961832Z",
     "shell.execute_reply.started": "2025-04-03T18:18:36.066557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing\n",
    "The next 3 cells:\n",
    "1. Load the preprocessed train and test data from CSV files\n",
    "2. Remove any unnamed index columns from both datasets\n",
    "3. Initialize iteration counter and confidence threshold for semi-supervised learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:18:37.964722Z",
     "iopub.status.busy": "2025-04-03T18:18:37.964081Z",
     "iopub.status.idle": "2025-04-03T18:18:38.040739Z",
     "shell.execute_reply": "2025-04-03T18:18:38.039630Z",
     "shell.execute_reply.started": "2025-04-03T18:18:37.964674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/datathon2025/final_preprocessed_train.csv\")\n",
    "test_data = pd.read_csv(\"/kaggle/input/datathon2025/final_preprocessed_test.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:18:38.090512Z",
     "iopub.status.busy": "2025-04-03T18:18:38.090094Z",
     "iopub.status.idle": "2025-04-03T18:18:38.097486Z",
     "shell.execute_reply": "2025-04-03T18:18:38.096353Z",
     "shell.execute_reply.started": "2025-04-03T18:18:38.090476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data.drop(columns=[\"Unnamed: 0\"], inplace=True, errors=\"ignore\")\n",
    "test_data.drop(columns=[\"Unnamed: 0\"], inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:18:38.098812Z",
     "iopub.status.busy": "2025-04-03T18:18:38.098536Z",
     "iopub.status.idle": "2025-04-03T18:18:38.118774Z",
     "shell.execute_reply": "2025-04-03T18:18:38.117606Z",
     "shell.execute_reply.started": "2025-04-03T18:18:38.098787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "confidence_threshold = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised Learning Implementation\n",
    "This notebook implements semi-supervised learning from scratch for a house price prediction task.\n",
    "The approach uses XGBoost with iterative self-training:\n",
    "1. Train initial model on labeled data\n",
    "2. Make predictions on unlabeled data\n",
    "3. Add high confidence predictions back to training set\n",
    "4. Repeat until convergence\n",
    "\n",
    "Key components:\n",
    "- Optuna for hyperparameter optimization\n",
    "- Bootstrap sampling for uncertainty estimation\n",
    "- Confidence thresholding for pseudo-labeling\n",
    "- Iterative training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:18:38.120276Z",
     "iopub.status.busy": "2025-04-03T18:18:38.119871Z",
     "iopub.status.idle": "2025-04-03T18:18:38.137540Z",
     "shell.execute_reply": "2025-04-03T18:18:38.136278Z",
     "shell.execute_reply.started": "2025-04-03T18:18:38.120214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500, step=50),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised Learning Loop\n",
    "This code implements an iterative semi-supervised learning approach:\n",
    "1. Trains an XGBoost model on labeled data using Optuna for hyperparameter tuning\n",
    "2. Makes predictions on unlabeled test data\n",
    "3. Uses bootstrap sampling to estimate prediction uncertainty\n",
    "4. Adds high confidence predictions back to training set\n",
    "5. Repeats until no more confident predictions remain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-03T18:58:50.739Z",
     "iopub.execute_input": "2025-04-03T18:18:38.138949Z",
     "iopub.status.busy": "2025-04-03T18:18:38.138643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "while not test_data.empty:\n",
    "    print(f\"Iteration {iteration + 1}: Hyperparameter tuning and training model...\")\n",
    "\n",
    "    # Prepare training data\n",
    "    X = train_data.drop(columns=[\"SalePrice\"])\n",
    "    y = train_data[\"SalePrice\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=200)  # Increase trials to 500\n",
    "    best_params = study.best_params\n",
    "    best_rmse = study.best_value\n",
    "    \n",
    "    print(f\"Best hyperparameters found: {best_params}\")\n",
    "    print(f\"Best RMSE obtained: {best_rmse}\")\n",
    "    \n",
    "    # Train final model with best hyperparameters\n",
    "    model = xgb.XGBRegressor(**best_params, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predict on test set\n",
    "    test_features = test_data.drop(columns=[\"SalePrice\", \"Confidence_Percentage\"], errors=\"ignore\")\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    # Bootstrap sampling for uncertainty estimation\n",
    "    n_samples = 40\n",
    "    bootstrap_preds = []\n",
    "    for _ in range(n_samples):\n",
    "        sample_data = resample(test_features)\n",
    "        preds = model.predict(sample_data)\n",
    "        bootstrap_preds.append(preds)\n",
    "    bootstrap_preds = np.array(bootstrap_preds)\n",
    "\n",
    "    # Compute mean and standard deviation\n",
    "    mean_predictions = bootstrap_preds.mean(axis=0)\n",
    "    std_predictions = bootstrap_preds.std(axis=0)\n",
    "\n",
    "    confidence_percentage = 100 * (1 - (std_predictions / (std_predictions.max() + 1e-6)))\n",
    "    confidence_percentage = np.clip(confidence_percentage, 0, 100)\n",
    "\n",
    "    # Store results\n",
    "    results_df = test_data.copy()\n",
    "    results_df[\"SalePrice\"] = mean_predictions\n",
    "    results_df[\"Confidence_Percentage\"] = confidence_percentage\n",
    "\n",
    "    # Select high-confidence predictions\n",
    "    filtered_df = results_df[results_df[\"Confidence_Percentage\"] > confidence_threshold].drop(columns=[\"Confidence_Percentage\"])\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"No more high-confidence samples. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # Update training and test data\n",
    "    train_data = pd.concat([train_data, filtered_df], ignore_index=True)\n",
    "    test_data = test_data.drop(filtered_df.index).reset_index(drop=True)\n",
    "    \n",
    "    # Print remaining test dataset size\n",
    "    print(f\"Remaining test dataset size: {len(test_data)}\")\n",
    "\n",
    "    train_data.to_csv(f\"final_train_data_ssl{iteration+1}.csv\", index=False)\n",
    "\n",
    "    # Save updated model\n",
    "    model_filename = f\"updated_xgb_model_iteration_{iteration + 1}.pkl\"\n",
    "    with open(model_filename, \"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f\"Model saved: {model_filename}\")\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "print(\"Semi-supervised learning process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7036244,
     "sourceId": 11265329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
